---
output: github_document
---

```{r global_options, include=FALSE}
library(knitr)
library(checkmate)
library(data.table)
opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)#, cache = TRUE)
options(width = 120)
set.seed(123)
```

# Install

To make use of functions from this package, you need to clone this repository, install the `devtools` R package, navigate to the directory of this package and use the `load_all()` function.

```{r}
# install.packages("devtools")
library(devtools)
load_all()
```

Install the development version from GitHub:

```{r, eval = FALSE}
# Currently, documentation is missing. Hence, installation may fail
# remotes::install_github("giuseppec/customtrees")
```

# Notes

- This package is not intended to be fast. It serves as a modular framework and playground to explore/study the splitting of features by custom objectives.
- Currently only trees of depth 1 are fitted with `split_parent_node`. If you want a tree, you need to call this function recursively on the generated child nodes. You can use `generate_node_index` to get the split indices of the observations from the current node. 
- Splits for categorical variables currently not implemented and tested. Try to handle categoricals as numerics as workaround. 
- The `perform_split` function computes (and aggregates) the objective in the generated nodes after splitting w.r.t. specific split points.
- Binary splits generate two nodes and are implemented in `find_best_binary_split`. The implementation  does exhaustive search of split point candidates to find the best split point for a given feature. 
- Multiple splits generate multiple nodes and are implemented in `find_best_multiway_split`. The implementation currently uses a slow simulated annealing optimization to find the best split point for a given feature (might be improved and replaced with other, faster optimization procedures).

# Define Objectives used as Split Criteria
```{r}
library(tidyverse)
library(Rmalschains)
library(dfoptim)
library(iml)
library(ranger)
library(kmlShape)
library(dtw)
```

```{r}
# objective that fits a constant in the nodes (CART) 
SS = function(y, x, requires.x = FALSE, ...) {
  ypred = mean(y)
  sum((y - ypred)^2)
}

# objective that fits a linear model in the nodes (mob)
SS_lm = function(y, x, requires.x = TRUE, ...) {
  ypred = predict(lm(y ~ x))
  sum((y - ypred)^2)
}

# point-wise L1 distance (is this frechet distance if grids are the same?)
SS_L1 = function(y, x, requires.x = FALSE, ...) {
  require(Rfast)
  ypred = Rfast::colMedians(as.matrix(y))
  sum(t(abs(t(y) - ypred)))
}

# point-wise L2 distance
SS_L2 = function(y, x, requires.x = FALSE, ...) {
  ypred = colMeans(y)
  sum(t((t(y) - ypred)^2))
}

# # point-wise L1 distance = frechet distance if grids are the same
# SS_L1 = function(y, x, requires.x = FALSE, ...) {
#   n = nrow(y)
#   center = colMeans(y)
#   centermat = t(replicate(n, center))
#   sum(abs(y - centermat))
# }

# Frechet distance FDA measure
SS_fre = function(y, x, requires.x = FALSE, ...) { # slow
  # using only y-axis of curves is enough as x-axis is always the same for all curves
  require(kmlShape)
  center = colMeans(y)
  grid.x = as.numeric(names(center))
  pdp.y = unname(center)
  dist = apply(y, 1, function(ice) distFrechet(grid.x, pdp.y, grid.x, ice, FrechetSumOrMax = "sum"))
  sum(dist)
}

# Dynamic time warping FDA measure
SS_dtw = function(y, x, requires.x = FALSE, ...) {
  require(dtw)
  pdp = colMeans(y) # this is the pdp
  dist = apply(y, 1, function(ice) dtw(ice, pdp, distance.only = TRUE)$normalizedDistance)
  sum(dist)
}
```

# CART with binary splits (constant model in node)

```{r}
nsim = 1000L
x = x = sort(runif(n = nsim, min = 0, max = 2*pi))
q = quantile(x, seq(0, 1, length.out = 100), type = 1)
y = ifelse(x > pi/2, rnorm(nsim, mean = 0), rnorm(nsim, mean = 10, sd = 2))
X = data.frame(x = x)

split = split_parent_node(y, X, objective = SS, optimizer = find_best_binary_split)
split

# plot result
plot(x, y)
abline(v = unlist(split$split.points))
```

# Extending CART to multiple splits (constant model in node)

```{r}
y = ifelse(x < pi/2, rnorm(nsim, mean = 0), 
  ifelse(x < pi, rnorm(nsim, mean = 10, sd = 2), 
    rnorm(nsim, mean = -10, sd = 5)))

# MA-LS Chains
split = split_parent_node(y, X, objective = SS, 
  optimizer = find_best_multiway_split_mals, n.splits = 2)

split

plot(x, y)
abline(v = unlist(split$split.points))
```

# MOB with binary splits (linear model in node)

```{r}
y = 4 + 2 * cos(x) + rnorm(nsim, mean = 0, sd = abs(cos(x)) / 2)

split = split_parent_node(y, X, objective = SS_lm, optimizer = find_best_binary_split, n.splits = 1)
split

plot(x, y)
abline(v = unlist(split$split.points))
```

# MOB with multiple splits (linear model in node)

```{r}
y = 4 + 2 * cos(x*2) + rnorm(nsim, mean = 0, sd = abs(cos(x)) / 2)

# MA-LS Chains
split = split_parent_node(y, X, objective = SS_lm, optimizer = find_best_multiway_split_mals, 
  n.splits = 3)

split

plot(x, y)
abline(v = unlist(split$split.points))
```

# Group ICE Curves with Multivariate Tree (binary splits, constant model in node)

We first generate some functional data:

```{r}
# Simulate Data
n = 500
x1 = round(runif(n, -1, 1), 1)
x2 = round(runif(n, -1, 1), 3)
x3 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5))
x4 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.7, 0.3))

# noisy vars
x5 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5))
x6 = rnorm(n, mean = 1, sd = 5)

y = 0.2*x1 - 8*x2 + ifelse(x1 < quantile(x1, 0.25), 8*x2, ifelse(x1 > quantile(x1, 0.75), 16*x2, 0))
#y = 0.2*x1 - 8*x2 + ifelse(x3 == 0, I(16*x2),0) + ifelse(x1 > mean(x1), I(8*x2),0)
# We also get interesting results using a 2-way interaction of numeric features
#y = 0.2*x1 - 8*x2 + 8*x6*x2
#y = 0.2*x1 - 8*x2^2 + 5*cos(x2*5)*x6 + ifelse(x3 == 0, I(8*x2),0)
eps = rnorm(n, 0, 0.1*sd(y))
y = y + eps

dat = data.frame(x1, x2, x3, x4, x5, x6, y)
X = dat[, setdiff(colnames(dat), "y")]

# Fit model and compute ICE for x2
mod = ranger(y ~ ., data = dat, num.trees = 500)
pred = function(model, newdata) predict(model, newdata)$predictions
model = Predictor$new(mod, data = X, y = dat$y, predict.function = pred)
effect = FeatureEffect$new(model, method = "ice", grid.size = 20, feature = "x2")

eff = as.data.table(effect$results)
# Center ICE curves
eff = as.data.frame(eff[, .value := (.value - mean(.value)), by = c(".type", ".id")])

# Plot ICE curves: WE WANT TO FIND SUBGROUPS SUCH THAT ICE KURVES ARE HOMOGENOUS
ggplot(eff, aes(x = x2, y = .value)) + 
  geom_line(aes(group = .id))
```

Formulate curves above by multivariate target and find feature that splits the curves such that they are more homogenous in the nodes:

```{r}
# Get ICE values and arrange them in a horizontal matrix
Y = spread(eff, x2, .value)
Y = Y[, setdiff(colnames(Y), c(".type", ".id"))]

str(X) # contains our feature values
str(Y) # contains ICE values for each grid point

sp = split_parent_node(Y = Y, X = X, objective = SS_L2,
  n.splits = 1, optimizer = find_best_binary_split)
sp
node_index = generate_node_index(Y, X, result = sp)
str(node_index)
```

```{r, echo = FALSE, eval = FALSE}
# Results here are only the same with SS_mah when ICE curves are NOT CENTERED!

# # objective for multivariate targets (multivariate tree), see MultivariateRandomForest::Node_cost function
# SS_mah = function(y, x, requires.x = FALSE, cov = NULL, ...) {
#   if (is.null(cov))
#     cov = cov(y)
#   center = colMeans(y)
#   # cov = cov(y) we need to pass the cov of all data
#   sum(mahalanobis(y, center = center, cov = cov, tol = 1e-30))
# }
#
# # compute covariance for data and use this in for mahalanobis distance in the objective
# COV = cov(Y)
# SS_mah2 = function(y, x, requires.x = FALSE, ...) 
#   SS_mah(y = y, x = x, requires.x = requires.x, cov = COV)
# sp = split_parent_node(Y = Y, X = X, objective = SS_mah2, 
#   n.splits = 1, optimizer = find_best_binary_split)
# sp
# node_index = generate_node_index(Y, X, result = sp)
# str(node_index)
#
# # frechet distance yields same splits but is a bit slower
# sp_frechet = split_parent_node(Y = Y, X = X, objective = SS_fre,
#   n.splits = 1, optimizer = find_best_binary_split)
# sp_frechet
# node_index_frechet = generate_node_index(Y, X, result = sp_frechet)
# str(node_index_frechet)
# 
# # dynamic time warping distance yields same splits but is much slower
# sp_dtw = split_parent_node(Y = Y, X = X, objective = SS_dtw,
#   n.splits = 1, optimizer = find_best_binary_split)
# sp_dtw
# node_index_dtw = generate_node_index(Y, X, result = sp_dtw)
# str(node_index_dtw)
# 
# # Compare with MultivariateRandomForest yields same result
# library(MultivariateRandomForest)
# invcov = solve(cov(Y), tol = 1e-30)
# sp2 = splitt2(X = as.matrix(X), Y = as.matrix(Y), m_feature = ncol(X), 
#   Index = 1:nrow(X), Inv_Cov_Y = invcov, Command = 2, ff = 1:ncol(X))
# str(sp2)
```

Visualize the results:

```{r}
plot.data = effect$results
plot.data$.split = node_index$class[plot.data$.id]

ggplot(plot.data, aes(x = x2, y = .value)) + 
  geom_line(aes(group = .id)) + facet_grid(~ .split)
```

# Group ICE Curves with Multivariate Tree (multiway splits, constant model in node)

<!-- Multiway split **fails** with `SS_mah2` (mahalanobis distance) as objective.  -->
<!-- This is because the curve structure along the x-axis is not considered in the distance calculation! -->

<!-- ```{r} -->
<!-- sp_multiway = split_parent_node(Y = Y, X = X, objective = SS_mah2,  -->
<!--   n.splits = 5, optimizer = find_best_multiway_split) -->
<!-- sp_multiway -->
<!-- node_index_multiway = generate_node_index(Y, X, result = sp_multiway) -->
<!-- str(node_index_multiway) -->

<!-- plot.data$.split = node_index_multiway$class[plot.data$.id] -->

<!-- ggplot(plot.data, aes(x = x2, y = .value)) +  -->
<!--   geom_line(aes(group = .id)) + facet_grid(~ .split) -->
<!-- ``` -->

Multiway splits using a distance measure suited for curves:

```{r}
sp_multiway = split_parent_node(Y = Y, X = X, objective = SS_L2, 
  n.splits = 2, optimizer = find_best_multiway_split_mals)
sp_multiway

node_index_multiway = generate_node_index(Y, X, result = sp_multiway)
str(node_index_multiway)

plot.data$.split = node_index_multiway$class[plot.data$.id]

ggplot(plot.data, aes(x = x2, y = .value)) + 
  geom_line(aes(group = .id)) + facet_grid(~ .split)
```

## Group ICE Curves with Multivariate Tree (multiway splits, constant model in node)

Now, we try a non-linear effect with a continuous interaction effect.

```{r, eval = TRUE}
y = 0.2*x1 - 8*x2^2 + 5*cos(x2*5)*x6 + eps
dat = data.frame(x1, x2, x3, x4, x5, x6, y)
X = dat[, setdiff(colnames(dat), "y")]

# Fit model and compute ICE for x2
mod = ranger(y ~ ., data = dat, num.trees = 1000)
pred = function(model, newdata) predict(model, newdata)$predictions
model = Predictor$new(mod, data = X, y = dat$y, predict.function = pred)
effect = FeatureEffect$new(model, method = "ice", grid.size = 20, feature = "x2")

eff = as.data.table(effect$results)
# Center ICE curves
eff = as.data.frame(eff[, .value := (.value - mean(.value)), by = c(".type", ".id")])
Y = spread(eff, x2, .value)
Y = Y[, setdiff(colnames(Y), c(".type", ".id"))]
#Y = as.data.frame(t(apply(Y, MARGIN = 1, function(x) x - mean(x))))

# Plot ICE curves: WE WANT TO FIND SUBGROUPS SUCH THAT ICE KURVES ARE HOMOGENOUS
ggplot(eff, aes(x = x2, y = .value)) + 
  geom_line(aes(group = .id))

sp_multi = lapply(1:4, function(i) {
  split_parent_node(Y = Y, X = X, objective = SS_L2, 
  n.splits = i, optimizer = find_best_multiway_split_mals, min.node.size = 10)
})
results = rbindlist(sp_multi, idcol = "n.splits")
results[results$best.split, ]

node_index_multiway = generate_node_index(Y, X, result = sp_multi[[length(sp_multi)]])
str(node_index_multiway)

plot.data = effect$results
plot.data$.split = node_index_multiway$class[plot.data$.id]

ggplot(plot.data, aes(x = x2, y = .value)) + 
  geom_line(aes(group = .id)) + facet_grid(~ .split)
```
