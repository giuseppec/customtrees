# REPID
# Split
#
# x: [vector] feature to search split
# y: [matrix] n x g matrix containing n ice curves computed on g grid points
# splits: vector of potential split.points to split x, default are 1% quantiles
search_split = function(x, y, splits = quantile(x, seq(0, 1, by = 1/100))) {
  checkmate::assert_vector(x)
  checkmate::assert_matrix(y)
  
  # sort feat
  ord = order(x)
  x = x[ord]
  y = y[ord,]
  N = nrow(y)
  
  cum_sum_y = Rfast::colCumSums(y)
  cum_sum_y2 = Rfast::colCumSums(y^2)
  
  objective = vapply(splits, function(split) {
    idx = which(x <= split)
    if (length(idx) == 0 || length(idx) == N) {
      return(Inf) # Invalid split
    }
    
    N_L = length(idx)
    
    S_L = cum_sum_y[N_L,]
    S_R = cum_sum_y[N,] - S_L
    
    SS_L = cum_sum_y2[N_L,]
    SS_R = cum_sum_y2[N,] - SS_L
    
    var_L = sum(SS_L - S_L^2 / N_L)
    var_R = sum(SS_R - S_R^2 / (N - N_L))
    
    return(var_L + var_R)
  }, FUN.VALUE = NA_real_)
  
  best = which.min(objective)
  split.points = splits[best]
  
  right = min(x[which(x > split.points)]) 
  left = max(x[which(x <= split.points)]) 
  
  data.frame(split.points = (left + right)/2, objective.value = objective[best])
}

best_split = function(X, y) {
  checkmate::assert_data_frame(X)
  res = data.table::rbindlist(lapply(X, function(x) search_split(x, y)))
  res$feature = colnames(X)
  res$best.split = res$objective == min(res$objective)
  return(res[, c("feature", "split.points", "objective.value", "best.split")])
}


# Example
library(data.table)
library(ranger)
library(iml)
library(tidyverse)

set.seed(1)

# Simulate Data
n = 50000
x1 = round(runif(n, -1, 1), 1)
x2 = round(runif(n, -1, 1), 3)
x3 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5))
x4 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.7, 0.3))

# noisy vars
x5 = sample(c(0, 1), size = n, replace = TRUE, prob = c(0.5, 0.5))
x6 = rnorm(n, mean = 1, sd = 5)

# target
y = 0.2*x1 - 8*x2 + ifelse(x3 == 0, I(16*x2),0) + ifelse(x1 > 0, I(8*x2),0)
eps = rnorm(n, 0, 0.1*sd(y))
y = y + eps

dat = data.frame(x1, x2, x3, x4, x5, x6, y)
X = dat[, setdiff(colnames(dat), "y")]

# Fit model and compute ICE for x2
mod = ranger(y ~ ., data = dat, num.trees = 500)
pred = function(model, newdata) predict(model, newdata)$predictions
model = Predictor$new(mod, data = X, y = dat$y, predict.function = pred)
effect = FeatureEffect$new(model, method = "ice", grid.size = 20, feature = "x2")

# Center ICE curves
eff = as.data.table(effect$results)
eff = as.data.frame(eff[, .value := (.value - mean(.value)), by = c(".type", ".id")])

# Plot ICE curves: WE WANT TO FIND SUBGROUPS SUCH THAT ICE CURVES ARE HOMOGENOUS
ggplot(eff, aes(x = x2, y = .value)) + 
  geom_line(aes(group = .id))

# Get ICE values and arrange them in a horizontal matrix
Y = spread(eff, x2, .value)
Y = Y[, setdiff(colnames(Y), c(".type", ".id"))]
Y = as.matrix(Y)

str(X) # contains our feature values
str(Y) # contains ICE values for each grid point

sp = best_split(X, Y)
sp

# Plot regions with ICE curves after best split
split_feat = sp$feature[sp$best.split]
split_val = sp$split.points[sp$best.split]
breaks = c(min(X[,split_feat]), split_val, max(X[,split_feat]))

split_region = cut(X[,split_feat], breaks = breaks, include.lowest = TRUE)
eff$.split = split_region[effect$results$.id]

ggplot(eff, aes(x = x2, y = .value)) + 
  geom_line(aes(group = .id)) + facet_grid(~ .split)







# point-wise L2 distance
SS_L2 = function(y, x, requires.x = FALSE, ...) {
  ypred = colMeans(y)
  sum(t((t(y) - ypred)^2))
}
sp = split_parent_node(Y = as.data.frame(Y), X = X, objective = SS_L2,
  n.splits = 1, optimizer = find_best_binary_split)
sp

plot.data = effect$results

node_index = generate_node_index(Y, X, result = sp)
str(node_index)
plot.data$.split = node_index$class[plot.data$.id]

ggplot(plot.data, aes(x = x2, y = .value)) + 
  geom_line(aes(group = .id)) + facet_grid(~ .split)

microbenchmark(split_parent_node(Y = as.data.frame(Y), X = X, objective = SS_L2,
  n.splits = 1, optimizer = find_best_binary_split), 
  best_split(X, Y), times = 10)


# Compare CART from CART_update.R at this point
# compute_variance_reductions(x1, Y[,1], generate_split_candidates(x1, n.quantiles = 100, min.node.size = 10))
# SS = function(y, x, requires.x = FALSE, ...) {
#   ypred = mean(y)
#   sum((y - ypred)^2)
# }
# split_parent_node(Y[,1], data.frame(X = x1), objective = SS, optimizer = find_best_binary_split)
#search_split(X$x1, Y, splits = generate_split_candidates(X$x1, n.quantiles = 100, min.node.size = 10))



